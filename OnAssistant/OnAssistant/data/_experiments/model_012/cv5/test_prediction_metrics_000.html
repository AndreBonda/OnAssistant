<!DOCTYPE html>
<head>
  <link rel="stylesheet" href="https://static2.sharepointonline.com/files/fabric/office-ui-fabric-core/9.6.0/css/fabric.min.css" />
  <script type="text/javascript" src="https://cdn.plot.ly/plotly-latest.min.js"></script>
  <script type="text/javascript" src="test_prediction_metrics_000.js"></script>
</head>
<body class="ms-Fabric ms-font-m">
<H3>Receiver Operating Characteristics (ROC)</H3><P>Receiver Operating Characteristics (ROC) curve is perhaps the most popular Machine Learning model evaluation plot. In one graph, it traces false postive rates (FPR) and true positive rates (i.e., TPR or recall) over a series of sorted score decision thresholds ranging from 0 to 1. Using a ROC curve, a user can easily see how a model performs and pick the optimal score decision threshold over the best trade-off between TPR and FPR. A ROC curve is usually concave, while a baseline random model plots a 45-degree roughly straight line (not really concave) from (0,0) to (1,1). The area-under-curve (AUC) value for this dummy model is close to 0.5, a baseline value for AUC-ROC. A reasonably better-than-random model can show a concave curve arching toward the (0,1) point and has an AUC value approaching 1, a perfect score for AUC-ROC. There are many more plots (shown below) for evaluating a ML model from many different angles. ROC and all these plots can help a user improve a ML model by carefully reviewing and revising the labeled set instances. From reviewing the ROC plot below, a user should pay attention to the categories/intents with a lower AUC value or a curve less arching toward (0,1). Detailed information about ROC and general model evaluation metrics can be found at <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A> and <A HREF="http://people.inf.elte.hu/kiss/11dwhdm/roc.pdf">An introduction to ROC analysis</A>.</P><DIV ID="ROCDiv"></DIV>
<H3>Precision Recall Curve (PRC)</H3><P>Precision and Recall Curve (PRC) is another popular Machine Learning model evaluation plot. It traces preicison and recall (i.e., TPR) together over a series of score decision thresholds. A user can review model performance and pick a threshold based on the best trade-off between preicison and recall. A good model usually has a curve arching towards the (1,1) point. For reference, see the Wikipedia page on <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="PRCDiv"></DIV>
<H3>True Positive Rate</H3><P>For some business, tracking true positive instances is more imporantant than relying an eclectic metric, like AUC-ROC. For example, recall (true positive rate) is very likely more important than precision for a ML model identifying cancerous moles (positive cases) and suggesting for a further review and check-up by a human physician. This plot shows TPR values over a series of score decision thresholds. A user can pick a decision threshold on this plot based on the TPR his/her business scenario demands. For reference, see the Wikipedia page on <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="TruePositiveRateDiv"></DIV>
<H3>False Positive Rate</H3><P>Similarly, some business may want to focus more on false positive instances. This FPR plot can help deciding the best score threshold as well. For reference, see the Wikipedia page on <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="FalsePositiveRateDiv"></DIV>
<H3>Precision</H3><P>Precision is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="PrecisionDiv"></DIV>
<H3>Recall</H3><P>Recall, i.e., true positive rate, is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="RecallDiv"></DIV>
<H3>Accuracy</H3><P>Accuracy is perhaps a familiar metric for most people, but it can be misleading for model evaluation, especially when a dataset (or population of data points) is skewed, i.e., having much more negatives than positives, or vice versa. For reference, see the Wikipedia page on <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="AccuracyDiv"></DIV>
<H3>True Negative Rate</H3><P>True nagative rate is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="TrueNegativeRateDiv"></DIV>
<H3>Negative Predictive Value</H3><P>Negative predictive value is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="NegativePredictiveValueDiv"></DIV>
<H3>False Negative Rate</H3><P>False negative rate is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="FalseNegativeRateDiv"></DIV>
<H3>False Discovery Rate</H3><P>False discovery rate is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="FalseDiscoveryRateDiv"></DIV>
<H3>False Omission Rate</H3><P>False omission rate is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="FalseOmissionRateDiv"></DIV>
<H3>Predicted Positive Rate</H3><P>Predicted positive rate is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="PredictedPositiveRateDiv"></DIV>
<H3>Predicted Negative Rate</H3><P>Predicted negative rate is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="PredictedNegativeRateDiv"></DIV>
<H3>F1 Measure</H3><P>F1 measure is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="F1MeasureDiv"></DIV>
<H3>F05 Measure</H3><P>F0.5 measure is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="F05MeasureDiv"></DIV>
<H3>F2 Measure</H3><P>F2 measure is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="F2MeasureDiv"></DIV>
<H3>Positive Likelihood Ratio</H3><P>Positive likelihood ratio is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="PositiveLikelihoodRatioDiv"></DIV>
<H3>Negative Likelihood Ratio</H3><P>Negative likelihood ratio is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="NegativeLikelihoodRatioDiv"></DIV>
<H3>Diagnostic Odds Ratio</H3><P>Diagnostic odds ratio is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="DiagnosticOddsRatioDiv"></DIV>
<H3>Matthews Correlation Coefficient</H3><P>Matthews Correlation Coefficient is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="MatthewsCorrelationCoefficientDiv"></DIV>
<H3>Bookmaker Informedness</H3><P>Bookmaker Informedness is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="BookmakerInformednessDiv"></DIV>
<H3>Markedness</H3><P>Markedness is a metric derived from <A HREF="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</A>.</P><DIV ID="MarkednessDiv"></DIV>
<H3>Reliability Diagram For Positive Instances Binned In Equal Number Of Entries</H3><P>A Reliability Diagram tracks the histogram of positive instance ratio over a set of score bins. It is mainly used to see if the positive ratio value in each bin is similar to the median score of the bin. If not, the scores predicted by a model cannot be used as probability. However, they can still be calibrated into probability. For this histogram, metric in each bin is aggregated over the same number of data instances.</P><DIV ID="ReliabilityDiagramForPositiveInstancesBinnedInEqualNumberOfEntriesDiv"></DIV>
<H3>Reliability Diagram For Negative Instances Binned In Equal Number Of Entries</H3><P>This Reliability Diagram plot tracks negative instnace ratio in each bin instead of postive. For this histogram, metric in each bin is aggregated over the same number of data instances.</P><DIV ID="ReliabilityDiagramForNegativeInstancesBinnedInEqualNumberOfEntriesDiv"></DIV>
<H3>Distribution Of Positive Instances Binned In Equal Number Of Entries</H3><P>This histogram is similar to the Reliability Diagram (for positives) above, but it shows number of positives instead of ratios.</P><DIV ID="DistributionOfPositiveInstancesBinnedInEqualNumberOfEntriesDiv"></DIV>
<H3>Distribution Of Negative Instances Binned In Equal Number Of Entries</H3><P>This histogram is similar to the Reliability Diagram (for negatives) above, but it shows number of negatives instead of ratios.</P><DIV ID="DistributionOfNegativeInstancesBinnedInEqualNumberOfEntriesDiv"></DIV>
<H3>Distribution Of Positive Negative Ratio Binned In Equal Number Of Entries</H3><P>This histogram tracks the ratio between positive instances over negatives.</P><DIV ID="DistributionOfPositiveNegativeRatioBinnedInEqualNumberOfEntriesDiv"></DIV>
<H3>Reliability Diagram For Positive Instances Binned In Equal Width</H3><P>This Reliability Diagram plot is similar to one shown above, but the histogram is built from equi-width bins. Some bin may have enough data points, so the histogram can fluctuate a lot if the test set is small.</P><DIV ID="ReliabilityDiagramForPositiveInstancesBinnedInEqualWidthDiv"></DIV>
<H3>Reliability Diagram For Negative Instances Binned In Equal Width</H3><P>This Reliability Diagram plot is similar to one shown above, but the histogram is built from equi-width bins.</P><DIV ID="ReliabilityDiagramForNegativeInstancesBinnedInEqualWidthDiv"></DIV>
<H3>Distribution Of Positive Instances Binned In Equal Width</H3><P>This histogram is similar to one shown above, but the histogram is built from equi-width bins.</P><DIV ID="DistributionOfPositiveInstancesBinnedInEqualWidthDiv"></DIV>
<H3>Distribution Of Negative Instances Binned In Equal Width</H3><P>This histogram is similar to one shown above, but the histogram is built from equi-width bins.</P><DIV ID="DistributionOfNegativeInstancesBinnedInEqualWidthDiv"></DIV>
<H3>Distribution Of Positive Negative Ratio Binned In Equal Width</H3><P>This histogram is similar to one shown above, but the histogram is built from equi-width bins.</P><DIV ID="DistributionOfPositiveNegativeRatioBinnedInEqualWidthDiv"></DIV>
<H3>Weighted Receiver Operating Characteristics (ROC)</H3><P>Weighted version for Receiver Operating Characteristics (ROC)</P><DIV ID="WeightedROCDiv"></DIV>
<H3>Weighted Precision Recall Curve (PRC)</H3><P>Weighted version for Precision Recall Curve (PRC)</P><DIV ID="WeightedPRCDiv"></DIV>
<H3>Weighted True Positive Rate</H3><P>Weighted version for True Positive Rate</P><DIV ID="WeightedTruePositiveRateDiv"></DIV>
<H3>Weighted False Positive Rate</H3><P>Weighted version for False Positive Rate</P><DIV ID="WeightedFalsePositiveRateDiv"></DIV>
<H3>Weighted Precision</H3><P>Weighted version for Precision</P><DIV ID="WeightedPrecisionDiv"></DIV>
<H3>Weighted Recall</H3><P>Weighted version for Recall</P><DIV ID="WeightedRecallDiv"></DIV>
<H3>Weighted Accuracy</H3><P>Weighted version for Accuracy</P><DIV ID="WeightedAccuracyDiv"></DIV>
<H3>Weighted True Negative Rate</H3><P>Weighted version for True Negative Rate</P><DIV ID="WeightedTrueNegativeRateDiv"></DIV>
<H3>Weighted Negative Predictive Value</H3><P>Weighted version for Negative Predictive Value</P><DIV ID="WeightedNegativePredictiveValueDiv"></DIV>
<H3>Weighted False Negative Rate</H3><P>Weighted version for False Negative Rate</P><DIV ID="WeightedFalseNegativeRateDiv"></DIV>
<H3>Weighted False Discovery Rate</H3><P>Weighted version for False Discovery Rate</P><DIV ID="WeightedFalseDiscoveryRateDiv"></DIV>
<H3>Weighted False Omission Rate</H3><P>Weighted version for False Omission Rate</P><DIV ID="WeightedFalseOmissionRateDiv"></DIV>
<H3>Weighted Predicted Positive Rate</H3><P>Weighted version for Predicted Positive Rate</P><DIV ID="WeightedPredictedPositiveRateDiv"></DIV>
<H3>Weighted Predicted Negative Rate</H3><P>Weighted version for Predicted Negative Rate</P><DIV ID="WeightedPredictedNegativeRateDiv"></DIV>
<H3>Weighted F1 Measure</H3><P>Weighted version for F1 Measure</P><DIV ID="WeightedF1MeasureDiv"></DIV>
<H3>Weighted F05 Measure</H3><P>Weighted version for F05 Measure</P><DIV ID="WeightedF05MeasureDiv"></DIV>
<H3>Weighted F2 Measure</H3><P>Weighted version for F2 Measure</P><DIV ID="WeightedF2MeasureDiv"></DIV>
<H3>Weighted Positive Likelihood Ratio</H3><P>Weighted version for Positive Likelihood Ratio</P><DIV ID="WeightedPositiveLikelihoodRatioDiv"></DIV>
<H3>Weighted Negative Likelihood Ratio</H3><P>Weighted version for Negative Likelihood Ratio</P><DIV ID="WeightedNegativeLikelihoodRatioDiv"></DIV>
<H3>Weighted Diagnostic Odds Ratio</H3><P>Weighted version for Diagnostic Odds Ratio</P><DIV ID="WeightedDiagnosticOddsRatioDiv"></DIV>
<H3>Weighted Matthews Correlation Coefficient</H3><P>Weighted version for Matthews Correlation Coefficient</P><DIV ID="WeightedMatthewsCorrelationCoefficientDiv"></DIV>
<H3>Weighted Bookmaker Informedness</H3><P>Weighted version for Bookmaker Informedness</P><DIV ID="WeightedBookmakerInformednessDiv"></DIV>
<H3>Weighted Markedness</H3><P>Weighted version for Markedness</P><DIV ID="WeightedMarkednessDiv"></DIV>
<H3>Weighted Reliability Diagram For Positive Instances Binned In Equal Number Of Entries</H3><P>Weighted version for Reliability Diagram For Positive Instances Binned In Equal Number Of Entries</P><DIV ID="WeightedReliabilityDiagramForPositiveInstancesBinnedInEqualNumberOfEntriesDiv"></DIV>
<H3>Weighted Reliability Diagram For Negative Instances Binned In Equal Number Of Entries</H3><P>Weighted version for Reliability Diagram For Negative Instances Binned In Equal Number Of Entries</P><DIV ID="WeightedReliabilityDiagramForNegativeInstancesBinnedInEqualNumberOfEntriesDiv"></DIV>
<H3>Weighted Distribution Of Positive Instances Binned In Equal Number Of Entries</H3><P>Weighted version for Distribution Of Positive Instances Binned In Equal Number Of Entries</P><DIV ID="WeightedDistributionOfPositiveInstancesBinnedInEqualNumberOfEntriesDiv"></DIV>
<H3>Weighted Distribution Of Negative Instances Binned In Equal Number Of Entries</H3><P>Weighted version for Distribution Of Negative Instances Binned In Equal Number Of Entries</P><DIV ID="WeightedDistributionOfNegativeInstancesBinnedInEqualNumberOfEntriesDiv"></DIV>
<H3>Weighted Distribution Of Positive Negative Ratio Binned In Equal Number Of Entries</H3><P>Weighted version for Distribution Of Positive Negative Ratio Binned In Equal Number Of Entries</P><DIV ID="WeightedDistributionOfPositiveNegativeRatioBinnedInEqualNumberOfEntriesDiv"></DIV>
<H3>Weighted Reliability Diagram For Positive Instances Binned In Equal Width</H3><P>Weighted version for Reliability Diagram For Positive Instances Binned In Equal Width</P><DIV ID="WeightedReliabilityDiagramForPositiveInstancesBinnedInEqualWidthDiv"></DIV>
<H3>Weighted Reliability Diagram For Negative Instances Binned In Equal Width</H3><P>Weighted version for Reliability Diagram For Negative Instances Binned In Equal Width</P><DIV ID="WeightedReliabilityDiagramForNegativeInstancesBinnedInEqualWidthDiv"></DIV>
<H3>Weighted Distribution Of Positive Instances Binned In Equal Width</H3><P>Weighted version for Distribution Of Positive Instances Binned In Equal Width</P><DIV ID="WeightedDistributionOfPositiveInstancesBinnedInEqualWidthDiv"></DIV>
<H3>Weighted Distribution Of Negative Instances Binned In Equal Width</H3><P>Weighted version for Distribution Of Negative Instances Binned In Equal Width</P><DIV ID="WeightedDistributionOfNegativeInstancesBinnedInEqualWidthDiv"></DIV>
<H3>Weighted Distribution Of Positive Negative Ratio Binned In Equal Width</H3><P>Weighted version for Distribution Of Positive Negative Ratio Binned In Equal Width</P><DIV ID="WeightedDistributionOfPositiveNegativeRatioBinnedInEqualWidthDiv"></DIV>
  <script>
    javascriptPlotlyFunction(); // ---- NOTE: not for node.js ----
   // ---- NOTE: for node.js ---- var javascriptPlotlyModule = require('test_prediction_metrics_000');
    // ---- NOTE: for node.js ---- var aJavascriptPlotlyModule = new javascriptPlotlyModule();
    // ---- NOTE: for node.js ---- aJavascriptPlotlyModule.javascriptPlotlyFunction();
  </script>
</body>

